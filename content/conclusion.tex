\section{Conclusion}

\par Dense Video Captioning is an extremely complex task that is can be considered as the culmination of various tasks in the field of Computer Vision as well as Natural Language Generation. It has numerous applications in different areas in the upcoming digital age, and so developing a solution for this problem is well motivated.

\par A performant DVC model requires multiple sequential modules, each with millions of parameters and numerous losses. We proposed a novel solution that improves upon the shortcomings of previous works by introducing multiple modalities, employing efficient attention mechanisms and using contextual learning to aid the end-to-end training paradigm. Multiple modalities lead to richer feature spaces compared to single modality features, while an end-to-end training paradigm promotes inter-task promotion for the proposal generation and captioning tasks. The use of attention mechanisms and transformer architectures for the task of DVC is relatively unexplored, hence we investigate the effectiveness of these for DVC.

\par First, we propose a feature extraction framework to generate temporally sensitive multimodal features using the Video Vision Transformer and Audio Spectrogram Transformer, providing a rich representation space for the model to work with. Along the way, we also evaluate the effectiveness of temporally sensitive features qualitatively and quantitatively.

\par Next, we propose a novel transformer-based architecture consisting of a encoder-decoder proposal generation module, a contextual learning module and a decoder-based captioning module. Each of these modules work in tandem to first encode the pre-trained multimodal features, followed by predicting the timestamps of all the events in the video along with a count of the number of events in the video. These parameters are then used to build a differentiable context mask which masks those tokens of the video that are not in the specific event under consideration. This aids in end-to-end training and backpropagates losses across the decoder as well (which would not have been possible if we used a static mask). Finally, the localized event features are fed into the caption decoder to predict next-work probabilites over the entire vocabulary.

\par We achieve competitive results against previous works over a range of evaluation metrics and provide detailed analysis and our qualitative interpretation on them. We also evaluate how well the model learns across a variety of settings and select the best one.

\par The challenges we faced during this project mainly stemmed from the huge sizes of video and audio data as well as the large models necessary for processing this data and predicting reasonable proposals and captions. These include high training time, memory constraints, and consequent problems in iterative improvement of our model. 	

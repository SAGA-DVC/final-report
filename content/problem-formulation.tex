\section{Problem Formulation}
\par The task of dense video captioning involves two sub-tasks (1) temporal localization of events in a video and (2) describing the localized events in natural language. 
Formally, the task is defined as follows:

\par \textbf{Given an input video $\mathbf{v} = \{v_1, v_2, ... , v_T \}$ where $v_i$ represents $i^{th}$ video frame in temporal order, the target of dense video captioning task is to output a set of sentences $\mathbf{S} = \{s_1, s_2, ... , s_{N_s}\}$ where $N_S$ is the number of sentences and $s_i = \{t_i^{start}, t_i^{end}, \{w_j\} \}$ consists of start and end timestamps for each sentence described with set of words from a vocabulary set $w_j \in \mathbf{V}$.}

\par Most of the architectures for dense video captioning comprises two components (1) Proposal generation module and (2) Captioning module. The modules can be trained in different ways like separate training, alternative training or in an end-to-end manner. The task of the proposal generation module is to take as input video frames $v$ and output event proposals $P = \{t_i^{start}, t_i^{end}, confidence_i \}$. Depending on the architecture, the proposal generation module can also output additional parameters like in \cite{krishna2017densecaptioning, li2018jointly, wang2018bidirectional} which can be utilized by the captioning module. The proposals can also be in the form of offsets (center and length) as in \cite{li2018jointly, iashin2020better, xu2018joint, deng2021sketch, zhou2018end}. The captioning module outputs descriptions for each of the proposed events in usually a single sentence.
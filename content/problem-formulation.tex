\subsection{Problem Formulation}
\par The task of dense video captioning involves two sub-tasks (1) temporal localization of events in a video and (2) describing the localized events in natural language. Given an input video $v = \{v_1, v_2, ... , v_T \}$ where $v_i$ represents $i^{th}$ video frame in temporal order, the target of dense video captioning task is to output a set of sentences $S = \{s_1, s_2, ... , s_{N_s}\}$ where $N_S$ is the number of sentences and $s_i = \{t_i^{start}, t_i^{end}, \{w_j\} \}$ consists of start and end timestamps for each sentence described with set of words from a vocabulary set $w_j \in V$.
\par Most of the architectures for dense video captioning comprises of two components (1) Proposal module and (2) Captioning module. The modules can be trained in different ways like separate training, alternative training or in a end-to-end manner. The task of Proposal module is to input video frames $v$ and output event proposals $P = \{t_i^{start}, t_i^{end}, confidence_i \}$. Depending on the architecture, the proposal module can also output additional parameters like in \cite{krishna2017densecaptioning, li2018jointly, wang2018bidirectional} which can be utilized by captioning module. The proposals can also be in the form of offsets (center and length) as in \cite{li2018jointly, iashin2020better, xu2018joint, deng2021sketch, zhou2018end}. The captioning module outputs descriptions for each of the proposed event in usually a single sentence.
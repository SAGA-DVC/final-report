\section{Objectives}

\par % TODO

\begin{itemize}
	\item \textbf{End to end training}: Develop an end-to-end framework for dense captioning of long untrimmed videos. The task of dense video captioning consists of two sub tasks, namely temporal proposal generation and proposal captioning. These two tasks are highly dependent on one another and training them independently or in a weakly dependent way like alternate training can lead to poor correlation between proposals and captions. The generated proposals should be able to generate good captions and the loss from generated captions should be able to backpropogate through the proposal generation module for correcting the timestamps. This backpropogation is only possible through an end-to-end training mechanism with strong relation between the two modules.
	\item \textbf{Multimodal input}: Utilize both video frame (visual) and audio features. Multimodal learning suggests that when a number of inputs – visual, auditory, kinaesthetic – are being used during learning, the model gets better understanding of the input data. Most of the current methods only utilize the visual modality from the videos. Taking inspiration from human perception system, which utilizes much more modalities than just the visual one for understanding the surrounding and making decisions, the model should exploit both audio and video modalities to take decisions on the proposal timestamps and captions.
	\item \textbf{Contextual learning}: Use temporal context while generating captions. The captions of the video are highly dependent on the time interval they correspond to. An event depends on the past, present and future events. In other words, there is existence of both inter-event and intra-event dependencies. The model should utilize these dependencies while predicting caption for an event.
\end{itemize}
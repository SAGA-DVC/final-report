
\par \textit{Machine learning} is a subfield of \textit{Artificial Intelligence}, which involves algorithms that enable automatic learning of tasks based on data. Unlike classical programming, which involves giving instructions and input data to a computer and producing output data, machine learning aims to produce the instructions or rules used for a task as the result, when the input and corresponding output data is provided to a computer.

\par \textit{Deep Learning}, a subfield of Machine Learning, has been growing popular in recent years, owing to their performance over traditional machine learning techniques without the need for intensive feature engineering by practitioners; deep learning models involve building up successive layers of increasingly meaningful representations automatically, allowing them to be rich and expressive with information. Different forms of deep learning models, including dense or fully connected layers, convolutional networks, recurrent networks and others have been applied to a variety of tasks which traditional algorithms have failed to address.

\par \textit{Convolutional networks (CNNs)} have dominated the field of computer vision for quite some time. Convolutional networks consist of convolutional layers, pooling layers and fully connected layers. Out of these, convolutional layers are the core building blocks of CNNs, which apply a series of different \textit{image filters} or \textit{convolutional kernels} to an input image in a convolution operation and the outputs of different filters at a layer are stacked together. The filter weights are learned, so as to learn to extract features while making use of the spatial structure inherent in images for the given task.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{assets/img/cnn.jpeg}
\caption{A Convolutional Network}
\label{fig:cnn}
\end{figure}

\par \textit{Transformers} were introduced by Vaswani \textit{et al} in \cite{tfm}. The transformer architecture is shown in Figure \ref{fig:transformer}. It consists of encoder layers which use \textit{multi-head self-attention} on input tokens followed by fully connected layers. The decoder layers use the output tokens and representations from encoder layers in a \textit{multi-head cross-attention} block followed by fully connected layers. This architecture has been adapted by various other models to serve a variety of tasks. Transformers are gaining traction in a variety of tasks, including sequence to sequence tasks like machine translation, natural language generation and understanding, and recently, computer vision.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{assets/img/transformer.png}
\caption{The Transformer architecture. Image courtesy \cite{tfm}}
\label{fig:transformer}
\end{figure}
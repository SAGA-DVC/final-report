\par The problem of dense video captioning was introduced by Krishna \textit{et al} in \cite{krishna2017densecaptioning} by proposing ActivityNet Captions dataset. Their architecture involved a proposal module and captioning module. The proposal module was inspired from DAPs \cite{Escorcia2016DAPsDA}, while the captioning module incorporated LSTM with contextual features along with event feature as inputs. The architecture was able to detect events and generate captions of the video in a single pass without the need of a time consuming sliding window approach.  However, since the features were dependent on the end location of the event, the model generated same captions for events ending at same timestamp.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{assets/img/timeline.jpg}
	\caption{Evolution of dense video captioning methods over time.}
\end{figure}

\par Following the release of ActivityNet Captions dataset in 2017, many researchers were able to surpass the results of baseline model and achieve state-of-the-art. Zhou \textit{et al} \cite{zhou2018end} addressed the problem of little influence of language desciptions on event proposals if the two modules are trained separately. They introduced an end-to-end masked transformer for propogating captioning error to proposal module for better performance. Furthermore, they proposed a self-attention mechanism for learning long-range dependencies in video. The proposal module was based on ProcNets \cite{zhou2017automatic}. The caption decoder employed a differentiable proposal mask to account for features in the respective event. Wang \textit{et al} \cite{wang2018bidirectional} employed Bi-SST as their proposal generator to account for past and future context information. The captioning module consisted of attentive fusion of context features, the weights of which were decided by a context gating mechanism. The architecture selected final captions based on joint ranking method which accounted for both proposal and caption confidence. Li \textit{et al} \cite{li2018jointly}, inspired by object localization networks like \cite{ren2016faster, liu2016ssd}, presented an end-to-end model with descriptiveness regression. An attribute-augmented LSTM network optimized using reinforcement learning was used for captioning module. Xiong \textit{et al} \cite{xiong2018forward} strived to generate relevant, coherent and concise descriptions using SSN \cite{zhao2017temporal} for event localization and LSTM for event selection and caption generation. Reinforcement learning with sentence-level reward is used to train the captioning network. Xu \textit{et al} \cite{xu2018joint} proposed JEDDi-Net, an end-to-end architecture incorporating visual and language contexts. Segment Proposal Network inspired from R-C3D \cite{xu2017rc3d} is used for proposal generation. Hierarchical LSTM with caption-level controller network and word-level sentence decoder is used for caption generation. The proposal features are represented using 3D Segment-of-Interest Pooling. Duan \textit{et al} \cite{duan2018weakly} introduced weak supervision training with no need of temporal annotations of events. They trained sentence localizer and caption generator in cyclic manner minimizing the reconstruction loss. However, the method struggles to detect beginning of events properly.

\par Mun \textit{et al} \cite{mun2019streamlined} tackles the challenge of coherent captioning by considering temporal dependency between events. They used SST for event proposals, PtrNet for event sequence generation and HRNN for captioning. Rahman \textit{et al} \cite{rahman2019watch} utilized weak supervision method from \cite{duan2018weakly} and were the first to try multi-modal approach for dense video captioning. They show how audio alone can be competitive to the previous visual based results. The paper also discusses various methods to encode audio (MFCC, CQT, SoundNet) and context fusion techniques for multiple modalities (Multiplicative Mixture, Multi-modal context fusion, MUTAN). The architecture suffered from proposal localization accuracy due to weak supervision. Furthermore, the results are also affected due to unavailability of part of the dataset as some videos are not available on their respective urls.

\par \sloppy Suin \textit{et al} \cite{suin2020efficient} aimed to reduce computational cost by processing fewer frames. They used deep reinforcement-based approach to describe multiple events in a video by watching a portion of the frames. The event proposal and captioning modules were inspired from \cite{zhou2018end}. Iashin \textit{et al} \cite{iashin2020multimodal} shows the importance of audio and speech modalities alongside visual features for dense video captioning task. They employ a Bi-SST for proposal and the captioning module consists of a Transformer architecture with three blocks: an encoder, decoder and generator. The model was able to achieve better performance than the then existing methods despite of unavailability of full dataset for multiple modalities. Iashin \textit{et al} \cite{iashin2020better} utilized audio and video with Bi-modal Transformer for captioning. The proposal generator consisted of multiheaded method, inspired from YOLO object detector \cite{yolo}. Their ablation analysis depicted stronger contribution of visual cues alone than audio cues alone. However, both modalities combined gave better results. Wang \textit{et al} \cite{wang2020densecaptioning} adapt DBG \cite{lin2019fast} for temporal event proposals alongwith ESGN \cite{mun2019streamlined} for candidate subset selection. The captioning module consists of an encoder-decoder architecture with CMG (cross-modal gating) block to adaptively balance the visual and linguistic information.

\par Chadha \textit{et al} \cite{chadha2020iperceive} proposed to handle cognitive error (causality between events) and incorrect attention (attending to objects in the frame). Their end-to-end model consisted of Bi-SST for proposals, Common-Sense Reasoning for causality learning and Transformer based architecture \cite{iashin2020multimodal} for captioning. The common-sense reasonining module employed a borrow-put experiment for determining dependency between events and generate context-aware features. Deng \textit{et al} \cite{deng2021sketch} introduced a top-down approach, reversing the usual detect-then-describe method. The architecture first generates a multi-sentence paragraph to describe the whole video and then localize each sentence for events. The captions are then refined using dual-path cross attention module. The top-down approach increased coherency in captions. Chen \textit{et al} \cite{chen2021towards} worked on closely bridging event localization and captioning modules for weakly supervised learning. The Induced Set Attention Block helps the captioner to learn highly abstracted global structure of the video. The method suffers from detecting visually small concepts/objects. Alwassel \textit{et al} \cite{alwassel2021tsp} introduce a supervised pre-training  paradigm for temporal action localization. The paradigm also considers background clips and global video information, to improve temporal sensitivity. Wang \textit{et al} \cite{wang2021endtoend} formulated dense video captioning as a set prediction task and introduced an end-to-end dense video captioning framework with parallel decoding (PDVC). PDVC adopts the vision transformer to learn attentive interaction of different frames. Two prediction heads run in parallel over query features, leveraging the mutual benefits between two tasks of localization and captioning.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
	\begin{tabular}{|c|c|cccc|cccc|}
		\hline
		\multirow{2}{*}{\textbf{Paper}}          & \multirow{2}{*}{\textbf{Set}} & \multicolumn{4}{c|}{\textbf{Ground Truth Proposals}}                                                                          & \multicolumn{4}{c|}{\textbf{Learnt Proposals}}                                                                       \\ \cline{3-10} 
		&                               & \multicolumn{1}{c|}{\textbf{M}} & \multicolumn{1}{c|}{\textbf{C}} & \multicolumn{1}{c|}{\textbf{B@3}} & \textbf{B@4}          & \multicolumn{1}{c|}{\textbf{M}} & \multicolumn{1}{c|}{\textbf{C}} & \multicolumn{1}{c|}{\textbf{B@3}} & \textbf{B@4} \\ \hline
		\multirow{2}{*}{Krishna \textit{et al} \cite{krishna2017densecaptioning} DCE}             & Validation                    & \multicolumn{1}{c|}{8.88}       & \multicolumn{1}{c|}{25.12}      & \multicolumn{1}{c|}{4.09}         & 1.60                  & \multicolumn{1}{c|}{5.69}       & \multicolumn{1}{c|}{12.43}      & \multicolumn{1}{c|}{1.90}         & 0.71         \\ \cline{2-10} 
		& Test                          & \multicolumn{1}{c|}{9.46}       & \multicolumn{1}{c|}{24.56}      & \multicolumn{1}{c|}{7.12}         & 3.98                  & \multicolumn{1}{c|}{4.82}       & \multicolumn{1}{c|}{17.29}      & \multicolumn{1}{c|}{3.86}         & 2.20         \\ \hline
		\multirow{2}{*}{Zhou \textit{et al} \cite{zhou2018end} Masked Transformer} & Validation                    & \multicolumn{1}{c|}{11.16}      & \multicolumn{1}{c|}{47.71}      & \multicolumn{1}{c|}{5.76}         & 2.71                  & \multicolumn{1}{c|}{4.98}       & \multicolumn{1}{c|}{9.25}       & \multicolumn{1}{c|}{2.42}         & 1.15         \\ \cline{2-10} 
		& Test                          & \multicolumn{1}{c|}{10.12}      & \multicolumn{1}{c|}{}           & \multicolumn{1}{l|}{}             & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{10.12}      & \multicolumn{1}{l|}{}           & \multicolumn{1}{c|}{}             &              \\ \hline
		\multirow{2}{*}{Wang \textit{et al} \cite{wang2018bidirectional} Bi-SST}             & Validation                    & \multicolumn{1}{c|}{10.89}      & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{}             &                       & \multicolumn{1}{c|}{5.86}       & \multicolumn{1}{c|}{7.99}       & \multicolumn{1}{c|}{2.55}         & 1.31         \\ \cline{2-10} 
		& Test                          & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{}           & \multicolumn{1}{l|}{}             & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{9.65}       & \multicolumn{1}{l|}{}           & \multicolumn{1}{c|}{}             &              \\ \hline
		\multirow{2}{*}{Li \textit{et al} \cite{li2018jointly} Descriptivness Regr.} & Validation                    & \multicolumn{1}{c|}{10.33}      & \multicolumn{1}{c|}{26.26}      & \multicolumn{1}{c|}{4.55}         & 1.71                  & \multicolumn{1}{c|}{6.93}       & \multicolumn{1}{c|}{13.21}      & \multicolumn{1}{c|}{2.27}         & 0.74         \\ \cline{2-10} 
		& Test                          & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{}           & \multicolumn{1}{l|}{}             & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{12.96}      & \multicolumn{1}{l|}{}           & \multicolumn{1}{c|}{}             &              \\ \hline
		Xu \textit{et al} \cite{xu2018joint} JEDDi-Net                             & Test                          & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{4.06}         & 1.63                  & \multicolumn{1}{c|}{8.58}       & \multicolumn{1}{c|}{19.88}      & \multicolumn{1}{c|}{4.06}         & 1.63         \\ \hline
		\multirow{2}{*}{Mun \textit{et al} \cite{mun2019streamlined} Streamlined DVC}     & Validation                    & \multicolumn{1}{c|}{13.07}      & \multicolumn{1}{c|}{43.48}      & \multicolumn{1}{c|}{4.41}         & 1.28                  & \multicolumn{1}{c|}{8.82}       & \multicolumn{1}{c|}{30.68}      & \multicolumn{1}{c|}{2.94}         & 0.93         \\ \cline{2-10} 
		& Test                          & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{}           & \multicolumn{1}{l|}{}             & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{8.19}       & \multicolumn{1}{l|}{}           & \multicolumn{1}{c|}{}             &              \\ \hline
		Duan \textit{et al} \cite{duan2018weakly} Weakly Supervised DCE               & Test                          & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{2.62}         & 1.27                  & \multicolumn{1}{c|}{6.3}        & \multicolumn{1}{c|}{18.77}      & \multicolumn{1}{c|}{2.62}         & 1.27         \\ \hline
		Rahman \textit{et al} \cite{rahman2019watch} Watch, Listen, Tell               & Validation *                  & \multicolumn{1}{c|}{7.23}       & \multicolumn{1}{c|}{25.36}      & \multicolumn{1}{c|}{3.04}         & 1.46                  & \multicolumn{1}{c|}{4.93}       & \multicolumn{1}{c|}{13.79}      & \multicolumn{1}{c|}{1.85}         & 0.9          \\ \hline
		Suin \textit{et al} \cite{suin2020efficient} Efficient framework for DVC         & Validation                    & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{2.87}         & 1.35                  & \multicolumn{1}{c|}{6.21}       & \multicolumn{1}{c|}{13.82}      & \multicolumn{1}{c|}{2.87}         & 1.35         \\ \hline
		Iashin \textit{et al} \cite{iashin2020multimodal} Multi-modal DVC                   & Validation *                  & \multicolumn{1}{c|}{11.72}      & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{5.83}         & 2.86                  & \multicolumn{1}{c|}{7.31}       & \multicolumn{1}{l|}{}           & \multicolumn{1}{c|}{2.6}          & 1.07         \\ \hline
		Iashin \textit{et al} \cite{iashin2020better} BMT                               & Validation *                  & \multicolumn{1}{c|}{10.90}      & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{4.63}         & 1.99                  & \multicolumn{1}{c|}{8.44}       & \multicolumn{1}{l|}{}           & \multicolumn{1}{c|}{3.84}         & 1.88         \\ \hline
		Xiong \textit{et al} \cite{xiong2018forward} Move Forward and Tell              & Validation                    & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{2.84}         & 1.24                  & \multicolumn{1}{c|}{7.08}       & \multicolumn{1}{l|}{}           & \multicolumn{1}{c|}{2.84}         & 1.24         \\ \hline
		\multirow{2}{*}{Wang \textit{et al} \cite{wang2020densecaptioning} SYSU}               & Validation                    & \multicolumn{1}{c|}{14.85}      & \multicolumn{1}{c|}{}           & \multicolumn{1}{l|}{}             & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{10.31}      & \multicolumn{1}{l|}{}           & \multicolumn{1}{c|}{}             &              \\ \cline{2-10} 
		& Test                          & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{}           & \multicolumn{1}{l|}{}             & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{9.28}       & \multicolumn{1}{l|}{}           & \multicolumn{1}{c|}{}             &              \\ \hline
		Chadha \textit{et al} \cite{chadha2020iperceive} iPerceive                         & Validation *                  & \multicolumn{1}{c|}{12.27}      & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{6.13}         & 2.98                  & \multicolumn{1}{c|}{7.87}       & \multicolumn{1}{l|}{}           & \multicolumn{1}{c|}{2.93}         & 1.29         \\ \hline
		Deng \textit{et al} \cite{deng2021sketch} Sketch, Ground and Refine           & Validation                    & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{}           & \multicolumn{1}{l|}{}             & 1.67                  & \multicolumn{1}{c|}{9.37}       & \multicolumn{1}{c|}{22.12}      & \multicolumn{1}{c|}{}             & 1.67         \\ \hline
		Chen \textit{et al} \cite{chen2021towards} Towards Bridging EC-SL              & Validation                    & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{2.78}         & 1.33                  & \multicolumn{1}{c|}{7.49}       & \multicolumn{1}{c|}{21.21}      & \multicolumn{1}{c|}{2.78}         & 1.33         \\ \hline
		\textbf{Alwassel \textit{et al} \cite{alwassel2021tsp} TSP with BMT}           & Validation                    & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{4.16}         & 2.02                  & \multicolumn{1}{c|}{8.75}       & \multicolumn{1}{l|}{}           & \multicolumn{1}{c|}{4.16}         & 2.02         \\ \hline
		Wang \textit{et al} \cite{wang2021endtoend} Parallel decoding                   & Validation *                  & \multicolumn{1}{c|}{11.26}      & \multicolumn{1}{c|}{53.65}      & \multicolumn{1}{l|}{}             & 3.12                  & \multicolumn{1}{c|}{8.08}       & \multicolumn{1}{c|}{28.59}      & \multicolumn{1}{c|}{}             & 1.96         \\ \hline
	\end{tabular}

\centering
\caption{Performance comparison of previous methods on the ActivityNet Captions dataset (* some videos unavailable)}  \label{tab: performance-comparison}

\end{table}

\section{Identified Themes}
\par Dense video captioning can be decomposed into two parts: event localization and event description. Existing research methodologies can be grouped into different categories based on training methods, modalities used and ordering of tasks.

\noindent\textbf{Based on Training schemes}:
\begin{enumerate}
	\item \textbf{Independent training}
	\item \textbf{Alternate training}: alternate between i) training the proposal module only and ii) training the captioning module on the positive event proposals while fine-tuning the proposal module.
	\item \textbf{End-to-End}
	\item \textbf{Weakly Supervised}
\end{enumerate}

\noindent\textbf{Based on Modalities}:
\begin{enumerate}
	\item \textbf{Uni-modal}: Only visual features
	\item \textbf{Multi-modal}: Combinations of visual, audio and speech features.
\end{enumerate}

\noindent\textbf{Based on Task ordering}:
\begin{enumerate}
	\item \textbf{Top-Down}: localize-then-describe
	\item \textbf{Bottom-Up}: describe-localize-refine
\end{enumerate}
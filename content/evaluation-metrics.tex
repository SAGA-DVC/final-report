\subsection{Proposal Evaluation}
Metrics for temporal action proposals are the same as that of object detection tasks. Following metrics are used to evaluate proposal predictions:

\subsubsection{mAP}
Understanding mAP requires knowledge about tIoU, precision and recall. tIoU (temporal Intersection over Union) is defined as the division of overlapping area between predicted interval and ground truth one, to that of union of both. tIoU threshold is used to decide if the predicted proposal is a correct prediction or not. Usually $tIoU > 0.5$ is considered as a correct prediction. Precision is used to calculate the percentage of accurate predictions $Precision = TP / (TP + FP)$. Recall measures the ability of the model to predict all ground truths $Recall = TP / (TP + FN)$. Average Precision is calculated by finding the area under precision-recall curve. PR curve is a plot of precision and recall at different tIoU thresholds. Average precision is calculated for each class in the dataset. Mean Average Precision(mAP) is the mean of AP over all classes. 

$$ mAP@tIoU = \frac{1}{n}\displaystyle\sum\limits_{i=1}^n AP_i \ ...for\ n\ classes $$

\subsection{Caption Evaluation}
Text Generation is a tricky domain. Academics as well as the industry still struggle for relevant metrics for evaluation of the generative models' qualities. Every generative task is different, having its own subtleties and peculiarities.These metrics can be applied to the numerous tasks such as:
\begin{itemize}
	\item Short or long-form text generation
	\item Machine Translation
	\item Summarisation
	\item Chatbots and dialogue systems
	\item Multimedia systems like speech2text, image/video captioning
\end{itemize}
Following are certain metrics that are used to evaluate the above natural language generation tasks:

\subsubsection{BLEU: Bilingual Evaluation Understudy}

\par BLEU \cite{bleu} is a precision focused metric that calculates n-gram overlap of the target and generated texts. This n-gram overlap means that this evaluation scheme is word-position independent apart from n-grams' term associations. BLEU also consists of a brevity penalty i.e. a penalty applied when the generated text is too small compared to the target text.


\subsubsection{ROUGE: Recall Oriented Understudy for Gisting Evaluation}

\par ROUGE \cite{rouge} is a set of metrics for evaluating automatic summarization of texts as well as machine translations. There are 3 types of ROUGE: ROUGE-N, the most common ROUGE type which means n-gram overlap. Second is ROUGE-L which checks for Longest Common Subsequence instead of n-gram overlap. The third is ROUGE-S which focuses on skip grams.


\subsubsection{METEOR: Metric for Evaluation of Translation with Explicit Ordering}

\par METEOR \cite{meteor} is an metric that works on word alignments. It computes one to one mapping of words in generated and reference texts. Traditionally, it uses WordNet or porter stemmer. Finally, it computes an F-score based on these mappings. The metric was designed to fix some of the problems found in the more popular BLEU metric, and also produce good correlation with human judgement at the sentence or segment level. This differs from the BLEU metric in that BLEU seeks correlation at the corpus level.


\subsubsection{CIDEr: Consensus based Image Description Evaluation}

\par CIDEr \cite{cider} is a metric used to evaluate image descriptions that uses human consensus. It uses a triplet-based method of collecting human annotations to measure consensus followed by stemming and grouping the words into n-grams. This metric is especially useful in image and video annoated tasks.


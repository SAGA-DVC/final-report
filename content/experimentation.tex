\section{Experimentation and Results}
\par We implement the feature extraction framework and the model architectures in PyTorch. We show that our model achieves competitive results compared to current state-of-the-art models with fewer training epochs and even fewer attention tokens. We utilize multimodal features from ViViT and AST encoders, pretrained using our adapted Temporally Sensitive Pretraining framework. The ActivityNet Captions dataset is used, which is the largest available dataset for the task of Dense Video Captioning.

\subsection{Feature Preprocessing Details}
\par We standardize all videos in the ActivityNet Captions dataset to 30 frames per second (30fps) and audio sampling rate of 44100Hz. For feature extraction as well as temporally sensitive pretraining, we use 16 frames per clip and sample 5 clips per segment in the dataset. We use 128 Mel bins and a target length of 64 for audio input. We start training the ViViT and AST encoders from Kinetics pretrained weights and Audioset pretrained weights respectively. ViViT uses a spatial patch size of $16 \times 16$, and a temporal patch size of 2. The number of encoder layers of ViViT and AST is kept 12. We train these encoders with a batch size of 8 and a stochastic gradient descent optimizer with momentum and weight decay.

\subsection {Training Details}
\par We train our model across 4 DGX-A100 GPUs for 70 epochs with an NCCL backend and a batch size of 16 for 60 hours. We use the AdamW optimizer with a learning rate of $10^{-4}$. We also use gradient clipping with a maximum threshold of $0.1$ to avoid exploding gradients. We set the model's embedding dimension to 512, with 6 layers and 8 attention heads for the transformer encoder, decoder and captioning module. Thus, our model has 75M parameters, excluding the feature extraction encoders. 
\par The smoothing rate $\alpha$ (for caption loss) is set to $0.5$. We initialize all transformer weights using the Xavier initialized distribution \cite{xavier}. We also use $sine$ positional embeddings, adapted for videos by accounting for their duration. We set the number of \textit{event queries} to 20 and the maximum event counter length to 10.
\par For the deformable attention head, we use 4 feature levels with 4 sampling sampling points per attention head and a 2048-dimensional FFN. We set $\rho$ to 0.5 (utilize only 50\% tokens) and dropout as 0.1.
\par For calculating the Hungarian loss, we perform a linear combination across all individual losses. Their corresponding coefficients are $\alpha_{l1}=5, \alpha_{giou}=2, \alpha_{event}=2, \alpha_m=3, \alpha_{cap}=1$.

\subsection {Evaluation Metrics}
\par We use 2 different metric sets to evaluate different aspects of our model.
\begin{itemize}
	\item For evaluating the event boundaries, we use Precision and Recall across IoU thresholds at $\left \{0.3, 0.5, 0.7, 0.9 \right\}$ along with an average across all the thresholds. We also calculate the corresponding average F1 score.
	\item For evaluating the captions, we calculate the BLEU \cite{bleu} scores across 4 thresholds at $\left \{1, 2, 3, 4\right\}$, METEOR \cite{meteor} and CIDEr \cite{cider}, based on the official ActivityNet challenge guidelines. The captions
\end{itemize}


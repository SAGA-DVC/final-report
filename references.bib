@misc{iashin2020better,
      title={A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer}, 
      author={Vladimir Iashin and Esa Rahtu},
      year={2020},
      eprint={2005.08271},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{yolo,
      title={YOLO9000: Better, Faster, Stronger}, 
      author={Joseph Redmon and Ali Farhadi},
      year={2016},
      eprint={1612.08242},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{carreira2018quo,
      title={Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset}, 
      author={Joao Carreira and Andrew Zisserman},
      year={2018},
      eprint={1705.07750},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{vit,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
      author={Alexey Dosovitskiy and
            Lucas Beyer and
            Alexander Kolesnikov and
            Dirk Weissenborn and
            Xiaohua Zhai and
            Thomas Unterthiner and
            Mostafa Dehghani and
            Matthias Minderer and
            Georg Heigold and
            Sylvain Gelly and
            Jakob Uszkoreit and
            Neil Houlsby},
      journal={CoRR},
      volume={abs/2010.11929},
      year={2020},
      url={https://arxiv.org/abs/2010.11929},
      eprinttype={arXiv},
      eprint={2010.11929},
      biburl={https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
      bibsource={dblp computer science bibliography, https://dblp.org}
}

@misc{tfm,
      title={Attention Is All You Need},
      author={Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
      journal={CoRR},
      volume={abs/1706.03762},
      year={2017},
      url={http://arxiv.org/abs/1706.03762},
      eprinttype={arXiv},
      eprint={1706.03762},
      biburl={https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
      bibsource={dblp computer science bibliography, https://dblp.org}
}

@article{bit,
  author    = {Alexander Kolesnikov and
               Lucas Beyer and
               Xiaohua Zhai and
               Joan Puigcerver and
               Jessica Yung and
               Sylvain Gelly and
               Neil Houlsby},
  title     = {Large Scale Learning of General Visual Representations for Transfer},
  journal   = {CoRR},
  volume    = {abs/1912.11370},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.11370},
  eprinttype = {arXiv},
  eprint    = {1912.11370},
  timestamp = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-11370.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{nos,
  author    = {Qizhe Xie and
               Eduard H. Hovy and
               Minh{-}Thang Luong and
               Quoc V. Le},
  title     = {Self-training with Noisy Student improves ImageNet classification},
  journal   = {CoRR},
  volume    = {abs/1911.04252},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.04252},
  eprinttype = {arXiv},
  eprint    = {1911.04252},
  timestamp = {Sun, 01 Dec 2019 20:31:34 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-04252.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{vivit,
  author= {Anurag Arnab and
           Mostafa Dehghani and
           Georg Heigold and
           Chen Sun and
           Mario Lucic and
           Cordelia Schmid},
  title= {ViViT: {A} Video Vision Transformer},
  journal= {CoRR},
  volume= {abs/2103.15691},
  year= {2021},
  url= {https://arxiv.org/abs/2103.15691},
  eprinttype= {arXiv},
  eprint= {2103.15691},
  timestamp= {Mon, 12 Apr 2021 13:42:11 +0200},
  biburl= {https://dblp.org/rec/journals/corr/abs-2103-15691.bib},
  bibsource= {dblp computer science bibliography, https://dblp.org}
}

@incollection{vggish,
title	= {CNN Architectures for Large-Scale Audio Classification},
author	= {Shawn Hershey and Sourish Chaudhuri and Daniel P. W. Ellis and Jort F. Gemmeke and Aren Jansen and Channing Moore and Manoj Plakal and Devin Platt and Rif A. Saurous and Bryan Seybold and Malcolm Slaney and Ron Weiss and Kevin Wilson},
year	= {2017},
URL	= {https://arxiv.org/abs/1609.09430},
booktitle	= {International Conference on Acoustics, Speech and Signal Processing (ICASSP)}
}

@misc{thumos-14,
   author = "Jiang, Y.-G. and Liu, J. and Roshan Zamir, A. and Toderici, G. and Laptev,
   I. and Shah, M. and Sukthankar, R.",
   title = "{THUMOS} Challenge: Action Recognition with a Large
   Number of Classes",
   howpublished = "\url{http://crcv.ucf.edu/THUMOS14/}",
   Year = {2014}}

@inproceedings{krishna2017densecaptioning,
    title={Dense-Captioning Events in Videos},
    author={Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Niebles, Juan Carlos},
    booktitle={International Conference on Computer Vision (ICCV)},
    year={2017}
}

@inproceedings{glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@inproceedings{large-scale-video-classification-cnn,
  title     = {Large-scale Video Classification with Convolutional Neural Networks},
  author    = {Andrej Karpathy and George Toderici and Sanketh Shetty and Thomas Leung and Rahul Sukthankar and Li Fei-Fei},
  year      = {2014},
  booktitle = {CVPR}
}
@misc{zhou2017automatic,
	title={Towards Automatic Learning of Procedures from Web Instructional Videos}, 
	author={Luowei Zhou and Chenliang Xu and Jason J. Corso},
	year={2017},
	eprint={1703.09788},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{he2015deep,
	title={Deep Residual Learning for Image Recognition}, 
	author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	year={2015},
	eprint={1512.03385},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{ren2016faster,
	title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}, 
	author={Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
	year={2016},
	eprint={1506.01497},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@article{zhou2018end,

title="End-to-End Dense Video Captioning with Masked Transformer",

author="Luowei {Zhou} and Yingbo {Zhou} and Jason J. {Corso} and Richard {Socher} and Caiming {Xiong}",

journal="arXiv preprint arXiv:1804.00819",

notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2795840542",

year="2018"

}

@misc{yao2015describing,
	title={Describing Videos by Exploiting Temporal Structure}, 
	author={Li Yao and Atousa Torabi and Kyunghyun Cho and Nicolas Ballas and Christopher Pal and Hugo Larochelle and Aaron Courville},
	year={2015},
	eprint={1502.08029},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@misc{venugopalan2015sequence,
	title={Sequence to Sequence -- Video to Text}, 
	author={Subhashini Venugopalan and Marcus Rohrbach and Jeff Donahue and Raymond Mooney and Trevor Darrell and Kate Saenko},
	year={2015},
	eprint={1505.00487},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{venugopalan2015translating,
	title={Translating Videos to Natural Language Using Deep Recurrent Neural Networks}, 
	author={Subhashini Venugopalan and Huijuan Xu and Jeff Donahue and Marcus Rohrbach and Raymond Mooney and Kate Saenko},
	year={2015},
	eprint={1412.4729},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{yu2016video,
	title={Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks}, 
	author={Haonan Yu and Jiang Wang and Zhiheng Huang and Yi Yang and Wei Xu},
	year={2016},
	eprint={1510.07712},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@inproceedings{Escorcia2016DAPsDA,
	title={DAPs: Deep Action Proposals for Action Understanding},
	author={Victor Escorcia and Fabian Caba Heilbron and Juan Carlos Niebles and Bernard Ghanem},
	booktitle={ECCV},
	year={2016}
}

@misc{chadha2020iperceive,
	title={iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering}, 
	author={Aman Chadha and Gurneet Arora and Navpreet Kaloty},
	year={2020},
	eprint={2011.07735},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{wang2018bidirectional,
	title={Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning}, 
	author={Jingwen Wang and Wenhao Jiang and Lin Ma and Wei Liu and Yong Xu},
	year={2018},
	eprint={1804.00100},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{li2018jointly,
	title={Jointly Localizing and Describing Events for Dense Video Captioning}, 
	author={Yehao Li and Ting Yao and Yingwei Pan and Hongyang Chao and Tao Mei},
	year={2018},
	eprint={1804.08274},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{rahman2019watch,
	title={Watch, Listen and Tell: Multi-modal Weakly Supervised Dense Event Captioning}, 
	author={Tanzila Rahman and Bicheng Xu and Leonid Sigal},
	year={2019},
	eprint={1909.09944},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{iashin2020multimodal,
	title={Multi-modal Dense Video Captioning}, 
	author={Vladimir Iashin and Esa Rahtu},
	year={2020},
	eprint={2003.07758},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{kay2017kinetics,
      title={The Kinetics Human Action Video Dataset}, 
      author={Will Kay and Joao Carreira and Karen Simonyan and Brian Zhang and Chloe Hillier and Sudheendra Vijayanarasimhan and Fabio Viola and Tim Green and Trevor Back and Paul Natsev and Mustafa Suleyman and Andrew Zisserman},
      year={2017},
      eprint={1705.06950},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{kuehne112011hmdb,
   author= "Kuehne, H. and Jhuang, H. and Garrote, E. and Poggio, T. and Serre, T.",
   title = "{HMDB}: a large video database for human motion recognition",
   booktitle = "Proceedings of the International Conference on Computer Vision (ICCV)",
   year = "2011",
}

@misc{soomro2012ucf101,
      title={UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild}, 
      author={Khurram Soomro and Amir Roshan Zamir and Mubarak Shah},
      year={2012},
      eprint={1212.0402},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{alwassel2021tsp,
      title={TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks}, 
      author={Humam Alwassel and Silvio Giancola and Bernard Ghanem},
      year={2021},
      eprint={2011.11479},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{xu2018joint,
	title={Joint Event Detection and Description in Continuous Video Streams}, 
	author={Huijuan Xu and Boyang Li and Vasili Ramanishka and Leonid Sigal and Kate Saenko},
	year={2018},
	eprint={1802.10250},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@INPROCEEDINGS{deng2021sketch,
	
	author={Deng, Chaorui and Chen, Shizhe and Chen, Da and He, Yuan and Wu, Qi},
	
	booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	
	title={Sketch, Ground, and Refine: Top-Down Dense Video Captioning}, 
	
	year={2021},
	
	volume={},
	
	number={},
	
	pages={234-243},
	
	doi={10.1109/CVPR46437.2021.00030}}


@article{aafaq2020video,
   title={Video Description},
   volume={52},
   ISSN={1557-7341},
   url={http://dx.doi.org/10.1145/3355390},
   DOI={10.1145/3355390},
   number={6},
   journal={ACM Computing Surveys},
   publisher={Association for Computing Machinery (ACM)},
   author={Aafaq, Nayyer and Mian, Ajmal and Liu, Wei and Gilani, Syed Zulqarnain and Shah, Mubarak},
   year={2020},
   month={Jan},
   pages={1–37}
}


@misc{bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@misc{rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@misc{meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}

@misc{cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}

@misc{wmd,
  title={From word embeddings to document distances},
  author={Kusner, Matt and Sun, Yu and Kolkin, Nicholas and Weinberger, Kilian},
  booktitle={International conference on machine learning},
  pages={957--966},
  year={2015},
  organization={PMLR}
}

@misc{spice,
  title={Spice: Semantic propositional image caption evaluation},
  author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  booktitle={European conference on computer vision},
  pages={382--398},
  year={2016},
  organization={Springer}
}

@misc{csn,
  author    = {Du Tran, Heng Wang, Lorenzo Torresani and Matt Feiszli},
  title     = {Video Classification with Channel-Separated Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1904.02811},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.02811},
  eprinttype = {arXiv},
  eprint    = {1904.02811},
  timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-02811.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{r(2+1)d,
  author    = {Du Tran and Heng Wang and Lorenzo Torresani and Jamie Ray and Yann LeCun and Manohar Paluri},
  title     = {A Closer Look at Spatiotemporal Convolutions for Action Recognition},
  journal   = {CoRR},
  volume    = {abs/1711.11248},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.11248},
  eprinttype = {arXiv},
  eprint    = {1711.11248},
  timestamp = {Mon, 13 Aug 2018 16:46:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-11248.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{vatt,
  title={Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text},
  author={Akbari, Hassan and Yuan, Linagzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  journal={arXiv preprint arXiv:2104.11178},
  year={2021}
}

@article{ast,
  title={AST: Audio Spectrogram Transformer},
  author={Gong, Yuan and Chung, Yu-An and Glass, James},
  journal={arXiv preprint arXiv:2104.01778},
  year={2021}
}

@article{liu2016ssd,
	title={SSD: Single Shot MultiBox Detector},
	ISBN={9783319464480},
	ISSN={1611-3349},
	url={http://dx.doi.org/10.1007/978-3-319-46448-0_2},
	DOI={10.1007/978-3-319-46448-0_2},
	journal={Lecture Notes in Computer Science},
	publisher={Springer International Publishing},
	author={Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year={2016},
	pages={21–37}
}

@misc{xiong2018forward,
	title={Move Forward and Tell: A Progressive Generator of Video Descriptions}, 
	author={Yilei Xiong and Bo Dai and Dahua Lin},
	year={2018},
	eprint={1807.10018},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{zhao2017temporal,
	title={Temporal Action Detection with Structured Segment Networks}, 
	author={Yue Zhao and Yuanjun Xiong and Limin Wang and Zhirong Wu and Xiaoou Tang and Dahua Lin},
	year={2017},
	eprint={1704.06228},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{xu2017rc3d,
	title={R-C3D: Region Convolutional 3D Network for Temporal Activity Detection}, 
	author={Huijuan Xu and Abir Das and Kate Saenko},
	year={2017},
	eprint={1703.07814},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{duan2018weakly,
	title={Weakly Supervised Dense Event Captioning in Videos}, 
	author={Xuguang Duan and Wenbing Huang and Chuang Gan and Jingdong Wang and Wenwu Zhu and Junzhou Huang},
	year={2018},
	eprint={1812.03849},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{mun2019streamlined,
	title={Streamlined Dense Video Captioning}, 
	author={Jonghwan Mun and Linjie Yang and Zhou Ren and Ning Xu and Bohyung Han},
	year={2019},
	eprint={1904.03870},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@article{suin2020efficient, title={An Efficient Framework for Dense Video Captioning}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6881}, DOI={10.1609/aaai.v34i07.6881}, abstractNote={&lt;p&gt;Dense video captioning is an extremely challenging task since an accurate and faithful description of events in a video requires a holistic knowledge of the video contents as well as contextual reasoning of individual events. Most existing approaches handle this problem by first proposing event boundaries from a video and then captioning on a subset of the proposals. Generation of dense temporal annotations and corresponding captions from long videos can be dramatically source consuming. In this paper, we focus on the task of generating a dense description of temporally untrimmed videos and aim to significantly reduce the computational cost by processing fewer frames while maintaining accuracy. Existing video captioning methods sample frames with a predefined frequency over the entire video or use all the frames. Instead, we propose a deep reinforcement-based approach which enables an agent to describe multiple events in a video by watching a portion of the frames. The agent needs to watch more frames when it is processing an informative part of the video, and skip frames when there is redundancy. The agent is trained using actor-critic algorithm, where the actor determines the frames to be watched from a video and the critic assesses the optimality of the decisions taken by the actor. Such an efficient frame selection simplifies the event proposal task considerably. This has the added effect of reducing the occurrence of unwanted proposals. The encoded state representation of the frame selection agent is further utilized for guiding event proposal and caption generation tasks. We also leverage the idea of knowledge distillation to improve the accuracy. We conduct extensive evaluations on ActivityNet captions dataset to validate our method.&lt;/p&gt;}, number={07}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Suin, Maitreya and Rajagopalan, A. N.}, year={2020}, month={Apr.}, pages={12039-12046} }

@misc{wang2020densecaptioning,
	title={Dense-Captioning Events in Videos: SYSU Submission to ActivityNet Challenge 2020}, 
	author={Teng Wang and Huicheng Zheng and Mingjing Yu},
	year={2020},
	eprint={2006.11693},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{lin2019fast,
	title={Fast Learning of Temporal Action Proposal via Dense Boundary Generator}, 
	author={Chuming Lin and Jian Li and Yabiao Wang and Ying Tai and Donghao Luo and Zhipeng Cui and Chengjie Wang and Jilin Li and Feiyue Huang and Rongrong Ji},
	year={2019},
	eprint={1911.04127},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@INPROCEEDINGS{chen2021towards,
	
	author={Chen, Shaoxiang and Jiang, Yu-Gang},
	
	booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	
	title={Towards Bridging Event Captioner and Sentence Localizer for Weakly Supervised Dense Event Captioning}, 
	
	year={2021},
	
	volume={},
	
	number={},
	
	pages={8421-8431},
	
	doi={10.1109/CVPR46437.2021.00832}}

@misc{wang2021endtoend,
	title={End-to-End Dense Video Captioning with Parallel Decoding}, 
	author={Teng Wang and Ruimao Zhang and Zhichao Lu and Feng Zheng and Ran Cheng and Ping Luo},
	year={2021},
	eprint={2108.07781},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@inproceedings{deit,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@INPROCEEDINGS{buch2017sst,

author={Buch, Shyamal and Escorcia, Victor and Shen, Chuanqi and Ghanem, Bernard and Niebles, Juan Carlos},

booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

title={SST: Single-Stream Temporal Action Proposals}, 

year={2017},

volume={},

number={},

pages={6373-6382},

doi={10.1109/CVPR.2017.675}}

@misc{lin2019bmn,
	doi = {10.48550/ARXIV.1907.09702},
	
	url = {https://arxiv.org/abs/1907.09702},
	
	author = {Lin, Tianwei and Liu, Xiao and Li, Xin and Ding, Errui and Wen, Shilei},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {BMN: Boundary-Matching Network for Temporal Action Proposal Generation},
	
	publisher = {arXiv},
	
	year = {2019},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{dosovitskiy2020vit,
	doi = {10.48550/ARXIV.2010.11929},
	
	url = {https://arxiv.org/abs/2010.11929},
	
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	
	publisher = {arXiv},
	
	year = {2020},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{liu2021swin,
	doi = {10.48550/ARXIV.2103.14030},
	
	url = {https://arxiv.org/abs/2103.14030},
	
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
	
	publisher = {arXiv},
	
	year = {2021},
	
	copyright = {Creative Commons Attribution 4.0 International}
}

@article{fayyaz2021ats,
	author = {Fayyaz, Mohsen and Kouhpayegani,  Soroush Abbasi and Jafari, Farnoush Rezaei and Sommerlade,  Eric and Vaezi Joze, Hamid and Pirsiavash,  Hamed and Gall, Juergen},
	title = {ATS: Adaptive Token Sampling For Efficient Vision Transformers},
	year = {2021},
	month = {December},
	abstract = {While state-of-the-art vision transformer models achieve promising results for image classification, they are computationally very expensive and require many GFLOPs. Although the GFLOPs of a vision transformer can be decreased by reducing the number of tokens in the network, there is no setting that is optimal for all input images. In this work, we, therefore, introduce a differentiable parameter-free Adaptive Token Sampling (ATS) module, which can be plugged into any existing vision transformer architecture. ATS empowers vision transformers by scoring and adaptively sampling significant tokens. As a result, the number of tokens is not anymore static but it varies for each input image. By integrating ATS as an additional layer within current transformer blocks, we can convert them into much more efficient vision transformers with an adaptive number of tokens. Since ATS is a parameter-free module, it can be added to off-the-shelf pretrained vision transformers as a plug-and-play module, thus reducing their GFLOPs without any additional training. However, due to its differentiable design, one can also train a vision transformer equipped with ATS. We evaluate our module on the ImageNet dataset by adding it to multiple state-of-the-art vision transformers. Our evaluations show that the proposed module improves the state-of-the-art by reducing the computational cost (GFLOPs) by 37% while preserving the accuracy.},
	url = {https://www.microsoft.com/en-us/research/publication/ats-adaptive-token-sampling-for-efficient-vision-transformers/},
	journal = {arXiv:2111.15667},
}

@misc{carion2020detr,
	doi = {10.48550/ARXIV.2005.12872},
	
	url = {https://arxiv.org/abs/2005.12872},
	
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {End-to-End Object Detection with Transformers},
	
	publisher = {arXiv},
	
	year = {2020},
	
	copyright = {Creative Commons Zero v1.0 Universal}
}

@misc{zhu2020deformable,
	doi = {10.48550/ARXIV.2010.04159},
	
	url = {https://arxiv.org/abs/2010.04159},
	
	author = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Deformable DETR: Deformable Transformers for End-to-End Object Detection},
	
	publisher = {arXiv},
	
	year = {2020},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{roh2021sparse,
	doi = {10.48550/ARXIV.2111.14330},
	
	url = {https://arxiv.org/abs/2111.14330},
	
	author = {Roh, Byungseok and Shin, JaeWoong and Shin, Wuhyun and Kim, Saehoon},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity},
	
	publisher = {arXiv},
	
	year = {2021},
	
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{zhou2018grounded,
	doi = {10.48550/ARXIV.1812.06587},
	
	url = {https://arxiv.org/abs/1812.06587},
	
	author = {Zhou, Luowei and Kalantidis, Yannis and Chen, Xinlei and Corso, Jason J. and Rohrbach, Marcus},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Grounded Video Description},
	
	publisher = {arXiv},
	
	year = {2018},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{songruc,
	title={A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer}, 
	author={Yuqing Song, Shizhe Chen, Yida Zhao, Qin Jin},
	year={2020},
	eprint={2006.07896},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}
@misc{xavier,
	title={Understanding the difficulty of training deep feedforward neural networks}, 
	author={Xavier Glorot,  Yoshua Bengio},
	year={2010},
	publisher =    {PMLR},
}

@misc{hungarian,
	title={The Dynamic Hungarian Algorithm for the Assignment Problem with Changing Costs},
	author={G. Ayorkor Mills-Tettey and Anthony Stentz and M. Bernardine Dias},
	year={2007}
}

